{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ethereum Smart Contracts Security Analysis**\n",
        "\n",
        "In this notebook we are going to analyze the state of the abilities of **LLMs** in the context of code generation, particularly for the generation of smart contracts in Solidity, in the Ethereum blockchain.\n",
        "\n",
        "---\n",
        "\n",
        "(*At this moment we are using a dataset and we are no more genrating the prompt ourself, but there's also the old code for prompt generation starting from a dataset containing smart contracts in solidity*)\n",
        "\n",
        "**Huggingface Dataset**: *https://huggingface.co/datasets/braindao/Solidity-Dataset*\n",
        "\n",
        "The steps will be the following:\n",
        "\n",
        "1.   *Import contracts and turn them into prompt* (**OPTIONAL**)\n",
        "\n",
        "1.   Import dataset containing the prompts for the generation\n",
        "\n",
        "2.   Setup OpenAI with API key\n",
        "\n",
        "3.   Setup a text to give as input to the LLM coder containing instructions for the generation\n",
        "\n",
        "4.   **Generate smart contracts**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H1aJQprbnGrt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGdzOt3oSWZV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#UNCOMMENT FIRST THREE LINES IF YOU ARE USING CONTRACT DATASET FOR PROMPT GENERATION\n",
        "\n",
        "#!unzip /content/drive/MyDrive/Ethereum_smart_contract_datast\n",
        "\n",
        "#import shutil\n",
        "#shutil.rmtree('/content/__MACOSX')\n",
        "#shutil.rmtree('/content/Ethereum_smart_contract_datast')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup OpenAI with his API key**"
      ],
      "metadata": {
        "id": "Y-0ifPZMsR1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F61ZV8_RRUdq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = \"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contract collection**\n",
        "\n",
        "The following cell is used if we are going to generate the prompts ourself.\n",
        "To do so we load a dataset taken from Github containing a huge amount of smart contracts written for Ethereum in Solidity. We collect a fixed number of contracts to save in local in order to pass them to the prompt generator.\n",
        "\n",
        "**YOU CAN SKIP IT IF YOU ARE USING A PROMPT DATASET**\n",
        "\n"
      ],
      "metadata": {
        "id": "lWrdMnUurq9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2FoS7HxBYDR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "contract_codes = []\n",
        "#The dataset is on my Google Drive\n",
        "source_directory = '/content/Ethereum_smart_contract_datast/contract_dataset_github/'\n",
        "codes_collected = 0\n",
        "codes_needed = 500 #To increase the dataset size\n",
        "\n",
        "for root, dirs, files in os.walk(source_directory):\n",
        "    for file in files:\n",
        "        if file.endswith('.sol'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            with open(file_path, 'r') as file:\n",
        "                contract_codes.append(file.read()) #Add the contract code to the array codes_collected\n",
        "                codes_collected += 1\n",
        "                if codes_collected >= codes_needed:\n",
        "                    break\n",
        "\n",
        "    #Stop we you reached the requested number of contracts\n",
        "    if codes_collected >= codes_needed:\n",
        "        break\n",
        "\n",
        "#print(f\"Contracts collected: {len(contract_codes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5QAOzKVB9NW"
      },
      "outputs": [],
      "source": [
        "#IF YOU WANT TO SEE ONE OF THE CONTRACTS COLLECTED\n",
        "index_to_print = 300\n",
        "\n",
        "if 0 <= index_to_print < len(contract_codes):\n",
        "    print(f\"Contract code {index_to_print + 1}:\")\n",
        "    print(contract_codes[index_to_print])\n",
        "else:\n",
        "    print(\"Invalid index\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_YqD6r3hT8c"
      },
      "source": [
        "## **PROMPT GENERATOR**\n",
        "\n",
        "In this section we proceed to generate the prompts for the LLMs.\n",
        "\n",
        "**YOU CAN SKIP IT IF YOU ARE USING A PROMPT DATASET**\n",
        "\n",
        "To do so we defined two different instructions, which gives different level of deepness to the contract generated. **At this moment** the preferred one is **instructions1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOEG8skomDDp"
      },
      "outputs": [],
      "source": [
        "instructions1 = \"\"\"\n",
        "Generate a prompt for an AI model for creating a contract starting from the smart contract code the user gives you.\n",
        "Underline that it should be in solidity\n",
        "The prompt should be a description of the contract, stating the purpose of it and its rules.\n",
        "If I give the prompt to another AI model it should generate pretty much the same contract.\n",
        "Specify the rules, the purpose and the general description of the code.\n",
        "\"\"\"\n",
        "\n",
        "instructions2 = \"\"\"\n",
        "Generate a prompt for creating a contract starting from the smart contract code the user gives you.\n",
        "Underline that it should be in solidity\n",
        "The prompt should be a description of the contract, stating the purpose of it and its rules.\n",
        "If I give the prompt to another AI model it should generate pretty much the same contract.\n",
        "It should be a simple description\n",
        "Not possible to modify or add anything with respect to the given prompt\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give to the generator two parameters:\n",
        "\n",
        "\n",
        "*   The instructions for the generation\n",
        "*   The smart contract that should be turned into prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "d6so53dQxW9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXLX0LKMc3th"
      },
      "outputs": [],
      "source": [
        "def prompt_generatorv1(smartcontract, generator_instructions):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\",\n",
        "        messages = [\n",
        "          {\"role\": \"system\", \"content\": generator_instructions},\n",
        "          {\"role\": \"user\", \"content\": smartcontract},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    generated_prompt = response.choices[0].message.content.strip()\n",
        "\n",
        "    return generated_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERATE 500 PROMPTS SELECTING A RANDOM CONTRACT AND TURNING IT INTO PROMPT**"
      ],
      "metadata": {
        "id": "dYAMcx8MyAQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WedDBoeWD5_4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "prompts = []\n",
        "how_many_prompts = 500\n",
        "\n",
        "for _ in range(how_many_prompts):\n",
        "    #Random contract selection\n",
        "    contract_index = random.randint(0, len(contract_codes) - 1)\n",
        "    selected_contract = contract_codes[contract_index]\n",
        "\n",
        "    prompts.append(prompt_generatorv1(selected_contract, instructions1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL USED TO PRINT A SELECTED PROMPT JUST TO CHECK**"
      ],
      "metadata": {
        "id": "yEwkeyDPyH_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce5I3Zp0F2-h"
      },
      "outputs": [],
      "source": [
        "index_to_print = 14\n",
        "\n",
        "if 0 <= index_to_print < len(prompts):\n",
        "    print(f\"My generated prompt {index_to_print + 1}:\")\n",
        "    print(prompts[index_to_print])\n",
        "else:\n",
        "    print(\"Invalid index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH0Q71yuMYlh"
      },
      "source": [
        "**PROMPT DATASET EXPORT AS A CSV FILE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKq7DK-7MJ0m"
      },
      "outputs": [],
      "source": [
        "data = {\"Prompt\": prompts}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#Exporting dataset\n",
        "csv_file_path = \"/content/test_prompts_dataset.csv\"  #UNDER CONTRUCTION\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"DATASET SAVED IN {csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**DATASET LOADING FROM HUGGINGFACE**\n",
        "\n",
        "In this section we load the dataset from HuggingFace, the link is available at the beginning of this notebook"
      ],
      "metadata": {
        "id": "eZFJ156URBBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "prompt_dataset = pd.read_parquet(\"hf://datasets/braindao/Solidity-Dataset/SolidityP.parquet\")\n",
        "#display(prompt_dataset[['average']])"
      ],
      "metadata": {
        "id": "xYydE_1GZL_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f5257c-73e4-42a1-ea9f-09e14783109c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL TO CHECK THE CONTENT OF THE DATASET, WE ARE USING COLUMN AVERAGE**"
      ],
      "metadata": {
        "id": "ajNOL1MC0ogG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_row = prompt_dataset.sample(n=1).iloc[0]\n",
        "print(sample_row['average'])"
      ],
      "metadata": {
        "id": "raMRN3Edvn5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2547ba92-0190-41fc-a232-97d740c62fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create a smart contract that follows the ERC20 token standard, allowing for token transfers, approvals, and burns. Use the SafeMath library for mathematical operations. Ensure that the contract tracks balances, allowances, and total supply accurately. Implement a constructor for initializing the contract name, symbol, and decimal places. Include functions for transferring tokens, approving spenders, and burning tokens. Focus on building a basic, functional contract that follows the ERC20 standard.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I save the first 250 prompts to use for the generation, in order to generate the same 250 contracts with both models"
      ],
      "metadata": {
        "id": "htrKHdXZ2fsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_to_generate = prompt_dataset['average'][:250].tolist()"
      ],
      "metadata": {
        "id": "7fTMt9nk02du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CODE GENERATION WITH GPT-4**\n",
        "\n",
        "In this section we proceed to actually generate the code for the smart contracts, using GPT4 model. To do so we define the instructions to give to the coder and we decide how many contracts we want to generate, in our case we are going to generate 500 smart contracts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JybNJMXltD5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmOrQ4qCOZLN"
      },
      "outputs": [],
      "source": [
        "#CODER INSTRUCTIONS\n",
        "\n",
        "coder = \"\"\"\n",
        "You will generate a deployable smart contract code in solidity, based on the prompt I give you.\n",
        "Use Solidity version ^0.8.0\n",
        "\n",
        "The file should contain only solidity code, no comments or \"```sol\".\n",
        "\n",
        "I should be able to copy your response and paste it in a sol file to deploy.\n",
        "Do not use Import statement, only code, if there's any import, replace it with code for the actual imported contract.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLDER FOR SOLIDITY CONTRACTS\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "output_dir = 'gpt_contracts/'\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "ibKDDX2oxC3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the manual checking we have seen that the code privided by chatgpt was not always ONLY CODE, but contained additional info or rows that we didn't want, so we created a function that removes everything above the two possible first lines, licence and pragma, and also everything after the last '}'"
      ],
      "metadata": {
        "id": "WDIfE-be6BA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_code(code):\n",
        "    #REMOVE EVERYTHING ABOVE // SPDX\n",
        "    spdx_index = code.find('// SPDX')\n",
        "    if spdx_index != -1:\n",
        "        code = code[spdx_index:]\n",
        "    else:\n",
        "        #IF THERE'S NO // SPDX, REMOVE EVERYTHING ABOVE pragma\n",
        "        pragma_index = code.find('pragma')\n",
        "        if pragma_index != -1:\n",
        "            code = code[pragma_index:]\n",
        "\n",
        "    #REMOVE EVERITHING AFTER LAST \"}\"\n",
        "    last_brace_index = code.rfind('}')\n",
        "    if last_brace_index != -1:\n",
        "        code = code[:last_brace_index + 1]\n",
        "\n",
        "    return code"
      ],
      "metadata": {
        "id": "soNLgqHx5x9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE GENERATOR PARAMETERS**\n",
        "\n",
        "*   **Coder instructions** defined above\n",
        "*   **Prompt** used for the generation\n",
        "\n"
      ],
      "metadata": {
        "id": "R-6npsZR01TM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTUk8VCINljF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "8bb00a88-a39c-49e8-fc67-916c7d10af62"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-11f892781313>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcontracts_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m \u001b[0;31m#SELECT NUMBER OF CONTRACTS NEEDED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontracts_to_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgpt_contract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_to_generate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprompt_used_gpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompts_to_generate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfile_name_gpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'contract_{i + 1}.sol'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-11f892781313>\u001b[0m in \u001b[0;36mcode_generator\u001b[0;34m(prompt, coder_instructions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcode_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoder_instructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         messages = [\n\u001b[1;32m      5\u001b[0m           \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoder_instructions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 942\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1003\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1080\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1032\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1080\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "def code_generator(prompt, coder_instructions):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-4\",\n",
        "        messages = [\n",
        "          {\"role\": \"system\", \"content\": coder_instructions},\n",
        "          {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    generated_contract = response.choices[0].message.content.strip()\n",
        "\n",
        "    return generated_contract\n",
        "\n",
        "generated_data = []\n",
        "\n",
        "contracts_to_generate = 250 #SELECT NUMBER OF CONTRACTS NEEDED\n",
        "for i in range(contracts_to_generate):\n",
        "    gpt_contract = sanitize_code(code_generator(prompts_to_generate[i], coder))\n",
        "    prompt_used_gpt = prompts_to_generate[i]\n",
        "    file_name_gpt = f'contract_{i + 1}.sol'\n",
        "\n",
        "    with open(f'/content/gpt_contracts/contract_{i + 1}.sol', 'w') as file:\n",
        "        file.write(sanitize_code(gpt_contract))\n",
        "\n",
        "    generated_data.append([prompt_used_gpt, gpt_contract, file_name_gpt])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERATE CSV FILE WITH DATA GENERATED**"
      ],
      "metadata": {
        "id": "Rfc_PZ986ASR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(generated_data, columns=['prompt_gpt', 'gpt_contract', 'file_name_gpt'])"
      ],
      "metadata": {
        "id": "quo__OHp6FiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPORT THE FOLDER OF GENERATED CONTRACTS**"
      ],
      "metadata": {
        "id": "buN4PVx51ZCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoOT4J72QBPT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('/content/gpt_contracts', 'zip', '/content/gpt_contracts')\n",
        "files.download('gpt_contracts.zip')\n",
        "\n",
        "\n",
        "print(\"The contracts have been saved and zipped successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CODE GENERATION WITH DEEPSEEK-CODER**\n",
        "\n",
        "In this section we proceed to actually generate the code for the smart contracts, using DeepSeek-Coder model. To do so we define the instructions to give to the coder and we decide how many contracts we want to generate, in our case we are going to generate 500 smart contracts"
      ],
      "metadata": {
        "id": "UpLFO4jJLXyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the opening \"```solidity\" and closing \"```\" delimiters\n",
        "\n",
        "def remove_code_delimiters(generated_code):\n",
        "    #Split into lines\n",
        "    lines = generated_code.split('\\n')\n",
        "\n",
        "    #Remove the opening delimiter if it's in the first line\n",
        "    if lines[0].strip() == \"```solidity\":\n",
        "        lines = lines[1:]\n",
        "\n",
        "    #Remove the opening delimiter if it's in the first line\n",
        "    if lines[0].strip() == \"```sol\":\n",
        "        lines = lines[1:]\n",
        "\n",
        "    #Remove the closing delimiter if it's in the last line\n",
        "    if lines[-1].strip() == \"```\":\n",
        "        lines = lines[:-1]\n",
        "\n",
        "    #Join the lines back together\n",
        "    cleaned_code = '\\n'.join(lines)\n",
        "    return cleaned_code"
      ],
      "metadata": {
        "id": "GPoweCb0UlGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLDER FOR SOLIDITY CONTRACTS\n",
        "import os\n",
        "\n",
        "output_dir = 'deepseek_contracts/'\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "EYr15x8aw0J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_api_endpoint = \"https://api.deepseek.com\"\n",
        "deepseek_api_key = \"sk-2cc98a8481304f04a847b46b5ffba937\"\n",
        "\n",
        "deepseek_coder_instructions = \"\"\" You will generate a deployable smart contract code in solidity, based on the prompt I give you.\n",
        "                                  Use Solidity version ^0.8.0\n",
        "\n",
        "                                  The contract should be made only in Solidity and it must be ready to deploy\n",
        "                                  only code , do not put ```solidity\" at the beginning of the response neither ``` at the end.\n",
        "                                  I want a fully deployable file with only code.\n",
        "\n",
        "                                  Do not use Import statement, only code, if there's any import, replace it with code for the actual imported contract.\n",
        "\n",
        "                                  \"\"\"\n",
        "\n",
        "deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=deepseek_api_endpoint)\n",
        "\n",
        "def code_generator_deepseek(prompt, coder_instructions):\n",
        "      response = deepseek_client.chat.completions.create(\n",
        "          model=\"deepseek-chat\",\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": coder_instructions},\n",
        "              {\"role\": \"user\", \"content\": prompt},\n",
        "          ],\n",
        "          stream=False\n",
        "      )\n",
        "\n",
        "      generated_contract = remove_code_delimiters(response.choices[0].message.content)\n",
        "\n",
        "      return generated_contract\n",
        "\n",
        "deepseek_data = []\n",
        "\n",
        "contracts_to_generate = 250 #SELECT NUMBER OF CONTRACTS NEEDED\n",
        "for i in range(contracts_to_generate):\n",
        "      ds_contract = remove_code_delimiters(code_generator_deepseek(prompts_to_generate[i], deepseek_coder_instructions))\n",
        "      prompt_used_ds = prompts_to_generate[i]\n",
        "      file_name_ds = f'contract_{i + 1}.sol'\n",
        "\n",
        "      with open(f'/content/deepseek_contracts/contract_{i + 1}.sol', 'w') as file:\n",
        "        file.write(ds_contract)\n",
        "\n",
        "      deepseek_data.append([prompt_used_ds, ds_contract, file_name_ds])"
      ],
      "metadata": {
        "id": "DX9LUXWPOmv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ADD THE CONTRACT GENERATED BY DEEPSEEK TO THE CSV**"
      ],
      "metadata": {
        "id": "GUYb1Vf_7SDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deepseek = pd.DataFrame(deepseek_data, columns=['prompt_deepseek', 'deepseek_contract', 'file_name_deepseek'])\n",
        "\n",
        "#combined_df = pd.concat([df, df_deepseek], axis=1)\n",
        "\n",
        "#combined_df.to_csv('/content/generated_contracts.csv', index=False)\n",
        "df_deepseek.to_csv('/content/deepseek.csv', index=False)"
      ],
      "metadata": {
        "id": "2_rsFpnQ7Q8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('/content/deepseek_contracts', 'zip', '/content/deepseek_contracts')\n",
        "files.download('deepseek_contracts.zip')\n",
        "files.download('/content/deepseek.csv')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The contracts have been saved and zipped successfully.\")"
      ],
      "metadata": {
        "id": "pwdHcGguwY6Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "bf40e4fb-8c1a-4b6b-d26f-6069ac23fb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e28673dd-56ce-4c08-8bf1-de76926210fd\", \"deepseek_contracts.zip\", 223567)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b3c66ac8-68f2-40d4-a0c9-3a124c1a5136\", \"deepseek.csv\", 864701)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The contracts have been saved and zipped successfully.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "lWrdMnUurq9n",
        "8_YqD6r3hT8c",
        "JybNJMXltD5N"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}